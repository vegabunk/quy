import streamlit as st
import cv2
import numpy as np
import io
import base64
import os
from PIL import Image, ImageDraw, ImageFont
from typing import Any
from ultralytics import YOLO
from ultralytics.utils.checks import check_requirements
from ultralytics.utils.downloads import GITHUB_ASSETS_STEMS
from ultralytics.utils import LOGGER
from ultralytics.utils.plotting import Annotator

# === Th√™m WebRTC ===
from streamlit_webrtc import webrtc_streamer, VideoProcessorBase, WebRtcMode
import av

# === C√°c h√†m cache g·ªëc ===
@st.cache_data
def phan_nguong(imgin: np.ndarray) -> np.ndarray:
    if len(imgin.shape) == 3:
        gray = cv2.cvtColor(imgin, cv2.COLOR_BGR2GRAY)
    else:
        gray = imgin.copy()
    M, N = gray.shape
    imgout = np.zeros((M, N), np.uint8)
    for x in range(M):
        for y in range(N):
            imgout[x, y] = 255 if gray[x, y] == 63 else 0
    return cv2.medianBlur(imgout, 7)

@st.cache_data
def predict_shape(imgin: np.ndarray) -> (np.ndarray, str):
    bin_img = phan_nguong(imgin)
    m = cv2.moments(bin_img)
    hu = cv2.HuMoments(m).flatten()
    h0 = hu[0]
    if 0.000620 <= h0 <= 0.000632:
        label = 'H√¨nh Tr√≤n'
    elif 0.000644 <= h0 <= 0.000668:
        label = 'H√¨nh Vu√¥ng'
    elif 0.000725 <= h0 <= 0.000751:
        label = 'H√¨nh Tam Gi√°c'
    else:
        label = 'Kh√¥ng x√°c ƒë·ªãnh'
    return bin_img, label

# === H√†m l·∫•y Base64 cho video n·ªÅn ===
def get_base64(path: str) -> str:
    with open(path, "rb") as f:
        return base64.b64encode(f.read()).decode()

# === CSS + Video background (GI·ªÆ NGUY√äN) ===
video_path = os.path.join(os.path.dirname(__file__), "resources/videos/background_5.mp4")
video_b64 = get_base64(video_path) if os.path.exists(video_path) else ""
css = f"""
<style>
  [data-testid="stSidebar"], [data-testid="stSidebarNav"] {{
    background: linear-gradient(135deg,#ccff99,#99ff99,#b2ff66,#66ff66,#99ff33,#33ff33,#80ff00,#00ff00) !important;
    height: 100vh; padding: 0;
  }}
  .video-bg {{
    position: fixed; top: 0; left: 0;
    width: 100%; height: 100vh;
    object-fit: cover; z-index: -1; opacity: 0.92;
  }}
  h1 {{
    color: white !important; text-shadow: 1px 1px 2px black;
  }}
  .caption-text {{
    font-size: 18px; font-weight: bold;
    color: white !important; margin-bottom: 5px;
    text-shadow: 1px 1px 2px black;
  }}
</style>
<video class="video-bg" autoplay muted loop>
  <source src="data:video/mp4;base64,{video_b64}" type="video/mp4">
</video>
"""
st.markdown(css, unsafe_allow_html=True)

# === Class Inference gi·ªØ nguy√™n ph·∫ßn sidebar + upload + configure ===
class Inference:
    def __init__(self, **kwargs: Any):
        check_requirements("streamlit>=1.29.0")
        st.title("üîç ·ª®ng d·ª•ng Ph√°t hi·ªán ƒë·ªëi t∆∞·ª£ng")
        self.conf = 0.25
        self.iou = 0.45
        self.source = None
        self.enable_trk = False
        self.org_frame = None
        self.ann_frame = None
        self.model = None
        self.vid_file_name = 0
        self.selected_ind = []
        self.temp = {"model": None, **kwargs}
        self.model_path = self.temp.get("model")

    def sidebar(self):
        with st.sidebar:
            st.title("C·∫•u h√¨nh")
            self.source = st.selectbox("Video t·ª´", ("webcam", "video"))
            self.enable_trk = st.radio("B·∫≠t tracking", ("Yes", "No"))
            self.conf = float(st.slider("Ng∆∞·ª°ng confidence", 0.0, 1.0, self.conf, 0.01))
            self.iou = float(st.slider("Ng∆∞·ª°ng IoU", 0.0, 1.0, self.iou, 0.01))
        col1, col2 = st.columns(2)
        self.org_frame = col1.empty()
        self.ann_frame = col2.empty()

    def source_upload(self):
        self.vid_file_name = 0
        if self.source == "video":
            vid = st.sidebar.file_uploader("Upload video", type=["mp4","mov","avi","mkv"])
            if vid is not None:
                content = io.BytesIO(vid.read())
                with open("ultra.mp4", "wb") as f:
                    f.write(content.read())
                self.vid_file_name = "ultra.mp4"

    def configure(self):
        models = [x.replace("yolo","YOLO") for x in GITHUB_ASSETS_STEMS if x.startswith("yolo11")]
        if self.model_path:
            models.insert(0, self.model_path.split(".pt")[0])
        sel = st.sidebar.selectbox("Ch·ªçn model", models)
        with st.spinner("Loading model..."):
            self.model = YOLO(f"{sel.lower()}.pt")
        names = list(self.model.names.values())
        cls = st.sidebar.multiselect("Ch·ªçn ƒë·ªëi t∆∞·ª£ng", names, default=names[:3])
        self.selected_ind = [names.index(c) for c in cls]

    def run(self):
        # 1) Sidebar + upload + configure
        self.sidebar()
        self.source_upload()
        self.configure()

        # 2) Start / Stop (GI·ªÆ NGUY√äN v·ªã tr√≠ n√∫t ch√≠nh gi·ªØa)
        if 'obj_running' not in st.session_state:
            st.session_state.obj_running = False
        col_s, col_t = st.columns(2)
        if col_s.button("Start"):
            st.session_state.obj_running = True
        if col_t.button("Stop"):
            st.session_state.obj_running = False

        # 3) N·∫øu ƒëang ch·∫°y:
        if st.session_state.obj_running:
            # --- WebRTC live (webcam) ---
            if self.source == "webcam":
                outer = self  # ƒë·ªÉ truy c·∫≠p trong l·ªõp

                class ObjectProcessor(VideoProcessorBase):
                    def recv(self, frame: av.VideoFrame) -> av.VideoFrame:
                        img_bgr = frame.to_ndarray(format="bgr24")
                        if outer.enable_trk == "Yes":
                            res = outer.model.track(
                                img_bgr, conf=outer.conf, iou=outer.iou,
                                classes=outer.selected_ind, persist=True
                            )
                        else:
                            res = outer.model(
                                img_bgr, conf=outer.conf, iou=outer.iou,
                                classes=outer.selected_ind
                            )
                        out_bgr = res[0].plot()
                        return av.VideoFrame.from_ndarray(out_bgr, format="bgr24")

                ctx = webrtc_streamer(
                    key="obj-detector",
                    mode=WebRtcMode.SENDRECV,
                    media_stream_constraints={"video": True, "audio": False},
                    video_processor_factory=ObjectProcessor,
                    async_processing=True,
                )
                if ctx.state.playing:
                    st.success("üé¨ ƒêang livestream v√† ph√°t hi·ªán ƒë·ªëi t∆∞·ª£ng‚Ä¶")
                else:
                    st.info("‚è∏Ô∏è Nh·∫•n ‚ñ∂Ô∏è ƒë·ªÉ b·∫Øt ƒë·∫ßu livestream")

            # --- Video file (gi·ªØ nguy√™n logic) ---
            else:
                cap = cv2.VideoCapture(self.vid_file_name)
                if not cap.isOpened():
                    st.error("Kh√¥ng m·ªü ngu·ªìn.")
                    return
                while cap.isOpened() and st.session_state.obj_running:
                    ok, frame = cap.read()
                    if not ok:
                        break
                    if self.enable_trk == "Yes":
                        res = self.model.track(
                            frame, conf=self.conf, iou=self.iou,
                            classes=self.selected_ind, persist=True
                        )
                    else:
                        res = self.model(
                            frame, conf=self.conf, iou=self.iou,
                            classes=self.selected_ind
                        )
                    out = res[0].plot()
                    self.org_frame.image(frame, channels="BGR")
                    self.ann_frame.image(out, channels="BGR")
                cap.release()
                cv2.destroyAllWindows()

# === C√°c ch·ª©c nƒÉng ·∫£nh c√≤n l·∫°i gi·ªØ nguy√™n ===
def fruit_detection():
    st.title("üçé Nh·∫≠n di·ªán v√† ph√¢n lo·∫°i tr√°i c√¢y")
    model = YOLO("yolo11n_trai_cay.pt", task="detect")
    buf = st.sidebar.file_uploader("Upload ·∫£nh", type=["bmp","png","jpg","jpeg","tif"])
    if buf is not None:
        img = Image.open(buf).convert("RGB")
        col1, col2 = st.columns(2)
        with col1:
            st.markdown("<div class='caption-text'>·∫¢nh g·ªëc</div>", unsafe_allow_html=True)
            st.image(img, use_container_width=True)
        if st.button("Process"):
            arr = np.array(img)
            bgr = cv2.cvtColor(arr, cv2.COLOR_RGB2BGR)
            res = model.predict(bgr, conf=0.5, verbose=False)
            boxes = res[0].boxes.xyxy.cpu()
            cls = res[0].boxes.cls.cpu().tolist()
            confs = res[0].boxes.conf.tolist()
            names = model.names
            ann = Annotator(bgr)
            for b, c, cf in zip(boxes, cls, confs):
                ann.box_label(b, f"{names[int(c)]} {cf:.2f}")
            out_bgr = ann.result()
            out_rgb = cv2.cvtColor(out_bgr, cv2.COLOR_BGR2RGB)
            with col2:
                st.markdown("<div class='caption-text'>·∫¢nh sau nh·∫≠n di·ªán</div>", unsafe_allow_html=True)
                st.image(out_rgb, use_container_width=True)
                st.download_button(
                    "üíæ T·∫£i ·∫£nh",
                    data=cv2.imencode('.jpg', cv2.cvtColor(out_rgb, cv2.COLOR_RGB2BGR))[1].tobytes(),
                    file_name="ketqua.jpg",
                    mime="image/jpeg"
                )

def shape_detection():
    st.title("üî∑ Nh·∫≠n di·ªán v√† ph√¢n lo·∫°i h√¨nh h·ªçc")
    uploaded = st.sidebar.file_uploader('Upload ·∫£nh g·ªëc', type=['png','jpg','jpeg','bmp'])
    if uploaded is not None:
        img = Image.open(uploaded).convert('RGB')
        img_np = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)
        col1, col2 = st.columns(2)
        with col1:
            st.markdown("<div class='caption-text'>·∫¢nh g·ªëc</div>", unsafe_allow_html=True)
            st.image(img, use_container_width=True)
        if st.button('Process'):
            bin_img, label = predict_shape(img_np)
            pil = Image.fromarray(cv2.cvtColor(img_np, cv2.COLOR_BGR2RGB))
            draw = ImageDraw.Draw(pil)
            try:
                font = ImageFont.truetype('resources/fonts/Arial.ttf', 40)
            except:
                font = ImageFont.load_default()
            draw.text((10, 30), label, font=font, fill=(0,255,0))
            with col2:
                st.markdown("<div class='caption-text'>·∫¢nh ch√∫ th√≠ch</div>", unsafe_allow_html=True)
                st.image(pil, use_container_width=True)
                st.markdown("<div class='caption-text'>·∫¢nh nh·ªã ph√¢n</div>", unsafe_allow_html=True)
                st.image(bin_img, use_container_width=True)

# === Main ===
def main():
    choice = st.sidebar.radio('Ch·ª©c nƒÉng', [
        'Ph√°t hi·ªán ƒë·ªëi t∆∞·ª£ng (video)',
        'Nh·∫≠n di·ªán v√† ph√¢n lo·∫°i tr√°i c√¢y (·∫£nh)',
        'Nh·∫≠n di·ªán v√† ph√¢n lo·∫°i h√¨nh h·ªçc (·∫£nh)'
    ])
    if choice == 'Ph√°t hi·ªán ƒë·ªëi t∆∞·ª£ng (video)':
        Inference().run()
    elif choice == 'Nh·∫≠n di·ªán v√† ph√¢n lo·∫°i tr√°i c√¢y (·∫£nh)':
        fruit_detection()
    else:
        shape_detection()

if __name__ == '__main__':
    main()
